# API Batch OpenAI - Guide d'utilisation

## Vue d'ensemble

L'API Batch d'OpenAI permet de traiter de grandes quantit√©s de requ√™tes de mani√®re asynchrone avec **50% de r√©duction sur les co√ªts**. Les r√©sultats sont disponibles sous 24 heures.

## Fonctionnalit√©s

### ‚úÖ Endpoints support√©s

- `/v1/chat/completions` - Conversations chat
- `/v1/completions` - Completion de texte
- `/v1/embeddings` - G√©n√©ration d'embeddings
- `/v1/moderations` - Mod√©ration de contenu
- `/v1/responses` - R√©ponses (Background mode)

### ‚úÖ Capacit√©s

- **Max 50,000 requ√™tes** par batch
- **Max 200 MB** par fichier
- **24h de traitement** garanties
- **50% de r√©duction** sur tous les tokens
- **Stockage local** de l'historique des batches

## Configuration

### 1. Cl√© API OpenAI

Ajoutez votre cl√© API dans `.env` :

```env
OPENAI_API_KEY=sk-votre-cle-api-openai
```

## Interface Web

**URL :** `http://localhost:5000/batch-api.html`

### Fonctionnalit√©s de l'interface :

- üìä **Statistiques** - Total batches, requ√™tes, tokens
- ‚ú® **Cr√©ation facile** - Mode simple et avanc√©
- üìã **Historique complet** - Liste tous les batches
- üîÑ **Actualisation auto** - Rafra√Æchissement toutes les 30s
- ‚¨áÔ∏è **T√©l√©chargement** - R√©cup√©ration des r√©sultats

## API Endpoints

### POST /api/batch/create

Cr√©e un nouveau batch.

**Options :**

1. **√Ä partir d'une liste de requ√™tes (recommand√©)**

```bash
curl -X POST http://localhost:5000/api/batch/create \
  -H "Content-Type: application/json" \
  -d '{
    "endpoint": "/v1/chat/completions",
    "completion_window": "24h",
    "requests": [
      {
        "custom_id": "request-1",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
          "model": "gpt-4o-mini",
          "messages": [
            {"role": "user", "content": "Quelle est la capitale de la France?"}
          ]
        }
      },
      {
        "custom_id": "request-2",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
          "model": "gpt-4o-mini",
          "messages": [
            {"role": "user", "content": "Explique la photosynth√®se"}
          ]
        }
      }
    ]
  }'
```

2. **√Ä partir d'un fichier d√©j√† upload√©**

```bash
curl -X POST http://localhost:5000/api/batch/create \
  -H "Content-Type: application/json" \
  -d '{
    "input_file_id": "file-abc123",
    "endpoint": "/v1/chat/completions",
    "completion_window": "24h",
    "metadata": {
      "project": "mon-projet",
      "batch_name": "test-batch-1"
    }
  }'
```

**R√©ponse :**

```json
{
  "success": true,
  "batch": {
    "id": "batch_abc123",
    "object": "batch",
    "endpoint": "/v1/chat/completions",
    "status": "validating",
    "input_file_id": "file-xyz789",
    "completion_window": "24h",
    "created_at": 1734691200,
    "request_counts": {
      "total": 0,
      "completed": 0,
      "failed": 0
    }
  }
}
```

### GET /api/batch/{batch_id}

R√©cup√®re les informations d'un batch.

```bash
curl http://localhost:5000/api/batch/batch_abc123?refresh=true
```

**Query params :**
- `refresh` : Si `true`, r√©cup√®re depuis l'API OpenAI (d√©faut: true)

**R√©ponse :**

```json
{
  "success": true,
  "batch": {
    "id": "batch_abc123",
    "status": "completed",
    "endpoint": "/v1/chat/completions",
    "output_file_id": "file-results123",
    "error_file_id": null,
    "created_at": 1734691200,
    "completed_at": 1734777600,
    "request_counts": {
      "total": 100,
      "completed": 95,
      "failed": 5
    },
    "usage": {
      "input_tokens": 1500,
      "output_tokens": 800,
      "total_tokens": 2300
    }
  }
}
```

### POST /api/batch/{batch_id}/cancel

Annule un batch en cours.

```bash
curl -X POST http://localhost:5000/api/batch/batch_abc123/cancel
```

**R√©ponse :**

```json
{
  "success": true,
  "batch": {
    "id": "batch_abc123",
    "status": "cancelling",
    "cancelling_at": 1734691500
  }
}
```

### GET /api/batch/list

Liste tous les batches.

```bash
# Liste depuis l'API OpenAI
curl http://localhost:5000/api/batch/list?limit=20&source=api

# Liste depuis la base locale
curl http://localhost:5000/api/batch/list?source=local&limit=50&status=completed
```

**Query params :**
- `source` : `api` (OpenAI) ou `local` (base de donn√©es)
- `limit` : Nombre max de batches (1-100)
- `after` : Cursor pour pagination (source=api)
- `status` : Filtrer par statut (source=local)
- `offset` : D√©calage pour pagination (source=local)

**R√©ponse :**

```json
{
  "success": true,
  "object": "list",
  "data": [
    {
      "id": "batch_abc123",
      "status": "completed",
      "endpoint": "/v1/chat/completions",
      "created_at": 1734691200
    }
  ],
  "first_id": "batch_abc123",
  "last_id": "batch_xyz789",
  "has_more": false
}
```

### GET /api/batch/stats

R√©cup√®re les statistiques des batches.

```bash
curl http://localhost:5000/api/batch/stats
```

**R√©ponse :**

```json
{
  "success": true,
  "stats": {
    "total_batches": 45,
    "by_status": {
      "completed": 30,
      "in_progress": 5,
      "failed": 2,
      "cancelled": 8
    },
    "request_counts": {
      "total": 45000,
      "completed": 43500,
      "failed": 1500
    },
    "usage": {
      "input_tokens": 1250000,
      "output_tokens": 850000,
      "total_tokens": 2100000
    }
  }
}
```

### GET /api/batch/{batch_id}/results

T√©l√©charge les r√©sultats d'un batch.

```bash
# R√©sultats r√©ussis
curl http://localhost:5000/api/batch/batch_abc123/results?type=output \
  -o results.jsonl

# R√©sultats en erreur
curl http://localhost:5000/api/batch/batch_abc123/results?type=error \
  -o errors.jsonl
```

**Query params :**
- `type` : `output` (r√©sultats) ou `error` (erreurs)

Le fichier t√©l√©charg√© est au format JSONL.

### POST /api/batch/upload

Upload un fichier JSONL.

```bash
curl -X POST http://localhost:5000/api/batch/upload \
  -F "file=@my_batch.jsonl" \
  -F "purpose=batch"
```

**R√©ponse :**

```json
{
  "success": true,
  "file": {
    "id": "file-abc123",
    "filename": "my_batch.jsonl",
    "bytes": 15234,
    "created_at": 1734691200,
    "purpose": "batch"
  }
}
```

## Format de fichier JSONL

Chaque ligne doit √™tre un objet JSON valide :

```jsonl
{"custom_id": "request-1", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o-mini", "messages": [{"role": "user", "content": "Hello!"}]}}
{"custom_id": "request-2", "method": "POST", "url": "/v1/chat/completions", "body": {"model": "gpt-4o-mini", "messages": [{"role": "user", "content": "Bonjour!"}]}}
```

### Structure d'une requ√™te :

```json
{
  "custom_id": "request-1",          // ID unique pour matching
  "method": "POST",                   // Toujours POST
  "url": "/v1/chat/completions",     // Endpoint
  "body": {                           // Param√®tres de la requ√™te
    "model": "gpt-4o-mini",
    "messages": [
      {"role": "user", "content": "Question..."}
    ],
    "max_tokens": 1000
  }
}
```

## Statuts des batches

- `validating` - Validation du fichier en cours
- `in_progress` - Traitement en cours
- `finalizing` - Finalisation des r√©sultats
- `completed` - Termin√© avec succ√®s
- `failed` - √âchec du batch
- `expired` - Expir√© (24h d√©pass√©es)
- `cancelling` - Annulation en cours
- `cancelled` - Annul√©

## Exemples d'utilisation

### Exemple 1 : Traduire 100 textes

```python
import requests
import json

# Pr√©parer les requ√™tes
requests_data = []
texts_to_translate = ["Hello", "Goodbye", "Thank you", ...]  # 100 textes

for i, text in enumerate(texts_to_translate):
    requests_data.append({
        "custom_id": f"translation-{i}",
        "method": "POST",
        "url": "/v1/chat/completions",
        "body": {
            "model": "gpt-4o-mini",
            "messages": [
                {"role": "system", "content": "Traduis en fran√ßais"},
                {"role": "user", "content": text}
            ]
        }
    })

# Cr√©er le batch
response = requests.post(
    "http://localhost:5000/api/batch/create",
    json={
        "endpoint": "/v1/chat/completions",
        "requests": requests_data
    }
)

batch = response.json()['batch']
print(f"Batch cr√©√©: {batch['id']}")
```

### Exemple 2 : G√©n√©rer des embeddings

```python
import requests

# Pr√©parer les textes
texts = ["Premier texte", "Deuxi√®me texte", ...]  # Max 50,000

requests_data = []
for i, text in enumerate(texts):
    requests_data.append({
        "custom_id": f"embedding-{i}",
        "method": "POST",
        "url": "/v1/embeddings",
        "body": {
            "model": "text-embedding-ada-002",
            "input": text
        }
    })

# Cr√©er le batch
response = requests.post(
    "http://localhost:5000/api/batch/create",
    json={
        "endpoint": "/v1/embeddings",
        "requests": requests_data,
        "metadata": {"project": "embeddings-generation"}
    }
)
```

### Exemple 3 : Surveiller et r√©cup√©rer les r√©sultats

```python
import requests
import time
import json

batch_id = "batch_abc123"

# V√©rifier le statut toutes les 5 minutes
while True:
    response = requests.get(f"http://localhost:5000/api/batch/{batch_id}")
    batch = response.json()['batch']
    
    print(f"Statut: {batch['status']}")
    print(f"Requ√™tes: {batch['request_counts']['completed']}/{batch['request_counts']['total']}")
    
    if batch['status'] == 'completed':
        # T√©l√©charger les r√©sultats
        results_response = requests.get(
            f"http://localhost:5000/api/batch/{batch_id}/results?type=output"
        )
        
        with open('results.jsonl', 'wb') as f:
            f.write(results_response.content)
        
        # Lire et traiter les r√©sultats
        with open('results.jsonl', 'r') as f:
            for line in f:
                result = json.loads(line)
                custom_id = result['custom_id']
                response_body = result['response']['body']
                print(f"{custom_id}: {response_body}")
        
        break
    
    elif batch['status'] in ['failed', 'expired', 'cancelled']:
        print("Batch termin√© sans succ√®s")
        break
    
    time.sleep(300)  # Attendre 5 minutes
```

## Bonnes pratiques

### 1. Optimiser les co√ªts

- Utilisez `gpt-4o-mini` pour 50% de r√©duction + batch = **75% de r√©duction totale**
- Groupez les requ√™tes similaires dans un m√™me batch
- Utilisez les batches pour les t√¢ches non urgentes

### 2. Gestion des erreurs

- Toujours v√©rifier `error_file_id` apr√®s completion
- Impl√©menter un retry pour les requ√™tes √©chou√©es
- Logger les `custom_id` pour tra√ßabilit√©

### 3. Performance

- Pr√©parez vos fichiers JSONL en avance
- Utilisez la base locale pour √©viter les appels API r√©p√©t√©s
- Surveillez l'expiration (24h apr√®s cr√©ation)

### 4. Limites

- Max 50,000 requ√™tes par batch
- Max 200 MB par fichier
- Pour embeddings : max 50,000 inputs par batch
- D√©lai de traitement : jusqu'√† 24h

## Base de donn√©es

Les batches sont stock√©s dans : `src/backend/data/batches.db`

### Tables :

- **batches** - Informations compl√®tes sur chaque batch
- **batch_files** - Fichiers upload√©s

## D√©pannage

### Batch en statut "validating" trop longtemps

- V√©rifiez le format JSONL (chaque ligne doit √™tre JSON valide)
- V√©rifiez que `custom_id` est unique pour chaque requ√™te
- V√©rifiez la taille du fichier (< 200 MB)

### Erreur "API key not configured"

- V√©rifiez que `OPENAI_API_KEY` est dans `.env`
- Red√©marrez le serveur apr√®s modification

### R√©sultats non disponibles

- Attendez que le statut soit `completed`
- V√©rifiez `output_file_id` est non-null
- Les r√©sultats sont disponibles pendant 1 an

## √âconomies r√©alis√©es

### Exemple de calcul :

**Sans batch :**
- 10,000 requ√™tes GPT-4o-mini
- 50,000 tokens input @ $0.150/1M = $7.50
- 30,000 tokens output @ $0.600/1M = $18.00
- **Total : $25.50**

**Avec batch :**
- M√™me volume avec 50% de r√©duction
- **Total : $12.75**
- **√âconomie : $12.75 (50%)**

## Support

Pour toute question :
- Documentation OpenAI : https://platform.openai.com/docs/api-reference/batch
- Interface web : `http://localhost:5000/batch-api.html`
- Logs : `logs/app.log`

## Limitations connues

- Pas de streaming en mode batch
- D√©lai de traitement variable (jusqu'√† 24h)
- N√©cessite une cl√© API OpenAI valide
- Les embeddings sont limit√©s √† 50,000 inputs

## Am√©liorations futures

- [ ] Notifications par email quand batch compl√©t√©
- [ ] Interface de cr√©ation de batch par drag & drop
- [ ] Export CSV des statistiques
- [ ] Retry automatique des requ√™tes √©chou√©es
- [ ] Planification de batches r√©currents
