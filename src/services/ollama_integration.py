"""
Module d'int√©gration des mod√®les IA locaux avec iaPostemanage
Utilise Ollama pour l'analyse de documents et l'assistance administrative
"""

import requests
import json
from typing import Optional, Dict, List
import logging

logger = logging.getLogger(__name__)


class OllamaClient:
    """Client pour interagir avec les mod√®les IA locaux via Ollama"""
    
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.models = {
            "general": "qwen2.5:32b",        # Meilleur pour documents fran√ßais
            "code": "deepseek-coder-v2:16b", # Meilleur pour code
            "fast": "mistral:7b-instruct",   # R√©ponses rapides
            "reasoning": "phi4",              # Raisonnement logique
            "analysis": "gemma2:9b"          # Analyse et cr√©ativit√©
        }
    
    def chat(self, 
             prompt: str, 
             model: str = None, 
             system: str = None,
             stream: bool = False) -> Dict:
        """
        Envoie un message au mod√®le IA
        
        Args:
            prompt: Le message/question √† envoyer
            model: Le mod√®le √† utiliser (par d√©faut: general)
            system: Instructions syst√®me pour le mod√®le
            stream: Si True, retourne un g√©n√©rateur
            
        Returns:
            Dict avec la r√©ponse du mod√®le
        """
        if model is None:
            model = self.models["general"]
        
        url = f"{self.base_url}/api/generate"
        
        data = {
            "model": model,
            "prompt": prompt,
            "stream": stream
        }
        
        if system:
            data["system"] = system
        
        try:
            response = requests.post(url, json=data)
            response.raise_for_status()
            
            if stream:
                return self._handle_stream(response)
            else:
                return response.json()
                
        except requests.exceptions.RequestException as e:
            logger.error(f"Erreur lors de la communication avec Ollama: {e}")
            return {"error": str(e)}
    
    def _handle_stream(self, response):
        """G√®re les r√©ponses en streaming"""
        for line in response.iter_lines():
            if line:
                yield json.loads(line)
    
    def analyze_document(self, text: str, document_type: str = "courrier") -> Dict:
        """
        Analyse un document administratif
        
        Args:
            text: Le texte du document √† analyser
            document_type: Type de document (courrier, contrat, facture, etc.)
            
        Returns:
            Dict avec l'analyse du document
        """
        system_prompt = f"""Tu es un assistant sp√©cialis√© dans l'analyse de documents administratifs fran√ßais.
Tu analyses des {document_type}s et extrais les informations importantes.
R√©ponds toujours en fran√ßais, de mani√®re claire et structur√©e."""
        
        prompt = f"""Analyse ce document et extrais les informations suivantes:
1. Type de document
2. Exp√©diteur et destinataire
3. Date
4. Objet principal
5. Points cl√©s
6. Actions requises
7. √âch√©ances importantes

Document:
{text}

Fournis une analyse structur√©e en JSON."""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["general"],
            system=system_prompt
        )
        
        return response
    
    def generate_response(self, 
                         context: str, 
                         request_type: str = "information") -> str:
        """
        G√©n√®re une r√©ponse de courrier automatique
        
        Args:
            context: Le contexte de la demande
            request_type: Type de demande (information, r√©clamation, etc.)
            
        Returns:
            str: La r√©ponse g√©n√©r√©e
        """
        system_prompt = """Tu es un assistant de r√©daction pour des courriers administratifs officiels fran√ßais.
Tu r√©diges des r√©ponses professionnelles, courtoises et conformes aux standards administratifs."""
        
        prompt = f"""R√©dige une r√©ponse de type '{request_type}' pour cette demande:

Contexte: {context}

La r√©ponse doit √™tre:
- Professionnelle et courtoise
- Claire et concise
- Conforme aux standards administratifs fran√ßais
- Inclure les formules de politesse appropri√©es"""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["general"],
            system=system_prompt
        )
        
        if "response" in response:
            return response["response"]
        return response.get("error", "Erreur de g√©n√©ration")
    
    def extract_entities(self, text: str) -> Dict:
        """
        Extrait les entit√©s nomm√©es d'un texte
        
        Args:
            text: Le texte √† analyser
            
        Returns:
            Dict avec les entit√©s extraites
        """
        prompt = f"""Extrais les entit√©s suivantes de ce texte:
- Personnes (noms, pr√©noms)
- Organisations
- Lieux
- Dates
- Montants
- R√©f√©rences (num√©ros de dossier, etc.)

Texte:
{text}

R√©ponds en JSON avec ces cat√©gories."""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["fast"]
        )
        
        return response
    
    def summarize(self, text: str, max_sentences: int = 3) -> str:
        """
        R√©sume un texte
        
        Args:
            text: Le texte √† r√©sumer
            max_sentences: Nombre maximum de phrases
            
        Returns:
            str: Le r√©sum√©
        """
        prompt = f"""R√©sume ce texte en maximum {max_sentences} phrases claires et informatives:

{text}"""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["fast"]
        )
        
        if "response" in response:
            return response["response"]
        return ""
    
    def classify_document(self, text: str) -> Dict:
        """
        Classifie un document selon son type
        
        Args:
            text: Le texte du document
            
        Returns:
            Dict avec le type et la confiance
        """
        prompt = f"""Classifie ce document parmi ces cat√©gories:
- Courrier officiel
- Facture
- Contrat
- R√©clamation
- Demande d'information
- Notification
- Attestation
- Autre

Document:
{text}

R√©ponds uniquement avec la cat√©gorie (une seule) et un score de confiance entre 0 et 1 en JSON."""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["analysis"]
        )
        
        return response
    
    def check_spelling(self, text: str) -> Dict:
        """
        V√©rifie l'orthographe et la grammaire
        
        Args:
            text: Le texte √† v√©rifier
            
        Returns:
            Dict avec les corrections sugg√©r√©es
        """
        prompt = f"""V√©rifie l'orthographe et la grammaire de ce texte fran√ßais.
Liste les erreurs trouv√©es et propose des corrections.

Texte:
{text}

R√©ponds en JSON avec: {{"erreurs": [{{"position": "...", "erreur": "...", "correction": "..."}}]}}"""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["general"]
        )
        
        return response
    
    def generate_code(self, description: str, language: str = "python") -> str:
        """
        G√©n√®re du code selon une description
        
        Args:
            description: Description de ce que le code doit faire
            language: Langage de programmation
            
        Returns:
            str: Le code g√©n√©r√©
        """
        prompt = f"""G√©n√®re du code {language} pour: {description}

Fournis uniquement le code, bien comment√© et suivant les bonnes pratiques."""
        
        response = self.chat(
            prompt=prompt,
            model=self.models["code"]
        )
        
        if "response" in response:
            return response["response"]
        return ""
    
    def list_models(self) -> List[str]:
        """Liste tous les mod√®les disponibles"""
        try:
            response = requests.get(f"{self.base_url}/api/tags")
            response.raise_for_status()
            models = response.json()
            return [m["name"] for m in models.get("models", [])]
        except Exception as e:
            logger.error(f"Erreur lors de la r√©cup√©ration des mod√®les: {e}")
            return []


# Instance globale pour faciliter l'utilisation
ollama = OllamaClient()


# Exemples d'utilisation
if __name__ == "__main__":
    # Test de connexion
    print("üîç Test de connexion √† Ollama...")
    models = ollama.list_models()
    print(f"‚úÖ Mod√®les disponibles: {models}\n")
    
    # Test d'analyse de document
    print("üìÑ Test d'analyse de document...")
    sample_text = """
    Monsieur le Directeur,
    
    Je vous √©cris pour faire suite √† votre courrier du 15 d√©cembre 2025 concernant 
    ma demande d'attestation fiscale pour l'ann√©e 2024.
    
    Je souhaiterais obtenir cette attestation dans les meilleurs d√©lais afin de 
    compl√©ter mon dossier de d√©claration d'imp√¥ts.
    
    Je vous prie d'agr√©er, Monsieur le Directeur, l'expression de mes salutations distingu√©es.
    
    Jean Dupont
    """
    
    result = ollama.summarize(sample_text, max_sentences=2)
    print(f"R√©sum√©: {result}\n")
    
    # Test de classification
    print("üè∑Ô∏è Test de classification...")
    classification = ollama.classify_document(sample_text)
    print(f"Classification: {classification}\n")
    
    # Test de g√©n√©ration de code
    print("üíª Test de g√©n√©ration de code...")
    code = ollama.generate_code("Fonction pour calculer la TVA √† partir d'un montant HT")
    print(f"Code g√©n√©r√©:\n{code}")
