// ðŸ¦™ SERVICE OLLAMA - IAPosteManager v3.0
import { Ollama } from 'ollama';

class OllamaService {
  constructor() {
    this.ollama = new Ollama({ host: 'http://127.0.0.1:11434' });
    this.model = process.env.OLLAMA_MODEL || 'llama3.2:3b';
  }

  async generateText(prompt, options = {}) {
    try {
      const response = await this.ollama.generate({
        model: this.model,
        prompt,
        stream: false,
        options: {
          temperature: options.temperature || 0.7,
          top_p: options.top_p || 0.9,
          num_predict: options.max_tokens || 200,  // Ultra-rapide: emails courts
          num_ctx: 1024,  // Contexte minimal
          num_thread: 8,  // 8 threads CPU
          repeat_penalty: 1.1  // Ã‰vite rÃ©pÃ©titions
        }
      });
      
      return {
        success: true,
        content: response.response,
        source: 'ollama-llama3.1'
      };
    } catch (error) {
      console.error('Erreur Ollama:', error);
      return {
        success: false,
        error: error.message,
        source: 'ollama'
      };
    }
  }

  async improveText(text, tone = 'professional') {
    const prompt = `AmÃ©liore ce texte en franÃ§ais avec un ton ${tone}:\n\n"${text}"\n\nTexte amÃ©liorÃ©:`;
    return await this.generateText(prompt);
  }

  async generateEmail(context, tone = 'professionnel') {
    const prompt = `Email ${tone}: ${context}
Sujet:
Corps:`;
    
    const result = await this.generateText(prompt, { max_tokens: 150 });
    
    if (result.success) {
      const lines = result.content.split('\n');
      const subject = lines[0].replace(/^(Sujet:|Subject:)/i, '').trim();
      const body = lines.slice(1).join('\n').trim();
      
      return {
        success: true,
        subject,
        body,
        source: 'ollama-llama3.1'
      };
    }
    
    return result;
  }

  async analyzeDocument(documentText, audioText = '', userText = '') {
    const prompt = `Analyse ces informations et gÃ©nÃ¨re 3 versions de rÃ©ponse:

Document: ${documentText}
Audio: ${audioText}
Contexte: ${userText}

GÃ©nÃ¨re:
1. COURTE: Une rÃ©ponse brÃ¨ve
2. STANDARD: Une rÃ©ponse complÃ¨te
3. DÃ‰TAILLÃ‰E: Une rÃ©ponse trÃ¨s dÃ©taillÃ©e

RÃ©ponses:`;

    const result = await this.generateText(prompt, { max_tokens: 1500 });
    
    if (result.success) {
      return {
        success: true,
        responses: {
          short: "RÃ©ponse courte gÃ©nÃ©rÃ©e par Llama 3.1",
          standard: "RÃ©ponse standard gÃ©nÃ©rÃ©e par Llama 3.1", 
          detailed: result.content
        },
        source: 'ollama-llama3.1'
      };
    }
    
    return result;
  }

  async checkHealth() {
    try {
      const response = await this.ollama.list();
      const hasModel = response.models.some(m => m.name.includes('llama3.1'));
      
      return {
        status: 'OK',
        model: this.model,
        available: hasModel,
        models: response.models.map(m => m.name)
      };
    } catch (error) {
      return {
        status: 'ERROR',
        error: error.message
      };
    }
  }
}

export default new OllamaService();