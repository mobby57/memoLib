# üìä Evals - √âvaluation des Performances IA

## √âvaluation des Mod√®les

### Cr√©er une √âvaluation Simple

```javascript
import { evalsAPI } from './services/api';

// Configuration de la source de donn√©es
const dataSourceConfig = {
  type: 'custom',
  schema: {
    type: 'object',
    properties: {
      item: {
        type: 'object',
        properties: {
          input: { type: 'string' },
          expected_output: { type: 'string' }
        },
        required: ['input', 'expected_output']
      }
    },
    required: ['item']
  }
};

// Crit√®res de test
const testingCriteria = [
  {
    type: 'string_check',
    name: 'Exact Match',
    input: '{{sample.output_text}}',
    reference: '{{item.expected_output}}',
    operation: 'eq'
  }
];

// Cr√©er l'√©valuation
const eval_ = await evalsAPI.create(
  'Test Email Generation',
  dataSourceConfig,
  testingCriteria,
  { description: 'Test de g√©n√©ration d\'emails' }
);
```

### Ex√©cuter une √âvaluation

```javascript
// Donn√©es de test
const testData = [
  {
    item: {
      input: '√âcris un email de remerciement client',
      expected_output: 'professional'
    }
  },
  {
    item: {
      input: 'Email de relance commercial',
      expected_output: 'persuasive'
    }
  }
];

// Source de donn√©es pour l'ex√©cution
const dataSource = {
  type: 'completions',
  source: {
    type: 'file_content',
    content: testData
  },
  input_messages: {
    type: 'template',
    template: [
      {
        role: 'developer',
        content: 'Tu es un assistant email professionnel.'
      },
      {
        role: 'user',
        content: '{{item.input}}'
      }
    ]
  },
  model: 'gpt-4o',
  sampling_params: {
    temperature: 0.7,
    max_completions_tokens: 1000
  }
};

// Lancer l'ex√©cution
const run = await evalsAPI.runs.create(eval_.id, dataSource, {
  name: 'Test Run 1'
});

console.log('Run ID:', run.id);
console.log('Status:', run.status);
```

## Service Email Evals

### √âvaluation Automatique des Emails

```javascript
import { emailEvalsAPI } from './services/api';

// Tester la qualit√© des emails
const emailPrompts = [
  {
    prompt: 'Email de bienvenue nouveau client',
    expected_tone: 'friendly',
    expected_length: 200
  },
  {
    prompt: 'Email de relance facture impay√©e',
    expected_tone: 'professional',
    expected_length: 150
  }
];

const results = await emailEvalsAPI.evaluateEmailQuality(
  emailPrompts,
  {
    checkTone: true,
    checkLength: true,
    checkProfessionalism: true,
    expectedTone: 'professional'
  }
);

console.log('R√©sultats:', {
  successRate: results.successRate,
  totalTests: results.totalTests,
  passed: results.passed,
  failed: results.failed
});
```

### Comparaison de Mod√®les

```javascript
// Comparer GPT-4o vs GPT-4o-mini
const comparison = await emailEvalsAPI.compareModels(
  emailPrompts,
  'gpt-4o',
  'gpt-4o-mini'
);

console.log('Comparaison:', {
  winner: comparison.comparison.winner,
  difference: comparison.comparison.difference,
  'gpt-4o': comparison['gpt-4o'].successRate,
  'gpt-4o-mini': comparison['gpt-4o-mini'].successRate
});
```

## Exemple Complet - Dashboard d'√âvaluation

```javascript
// Composant React pour dashboard d'√©valuation
import { useState, useEffect } from 'react';
import { evalsAPI, emailEvalsAPI } from '../services/api';

function EvalDashboard() {
  const [evals, setEvals] = useState([]);
  const [currentRun, setCurrentRun] = useState(null);
  const [results, setResults] = useState(null);
  const [isRunning, setIsRunning] = useState(false);

  useEffect(() => {
    loadEvals();
  }, []);

  const loadEvals = async () => {
    try {
      const response = await evalsAPI.list({ limit: 10 });
      setEvals(response.data);
    } catch (error) {
      console.error('Erreur chargement evals:', error);
    }
  };

  const runEmailEval = async () => {
    setIsRunning(true);
    
    try {
      const testEmails = [
        {
          prompt: 'Email de confirmation commande',
          expected_tone: 'professional',
          expected_length: 200
        },
        {
          prompt: 'Email de promotion produit',
          expected_tone: 'persuasive',
          expected_length: 300
        }
      ];

      const evalResults = await emailEvalsAPI.evaluateEmailQuality(
        testEmails,
        { expectedTone: 'professional' }
      );

      setResults(evalResults);
    } catch (error) {
      console.error('Erreur √©valuation:', error);
    } finally {
      setIsRunning(false);
    }
  };

  const compareModels = async () => {
    setIsRunning(true);
    
    try {
      const testPrompts = [
        { prompt: 'Email service client', expected_tone: 'helpful' },
        { prompt: 'Email marketing', expected_tone: 'engaging' }
      ];

      const comparison = await emailEvalsAPI.compareModels(
        testPrompts,
        'gpt-4o',
        'gpt-4o-mini'
      );

      setResults(comparison);
    } catch (error) {
      console.error('Erreur comparaison:', error);
    } finally {
      setIsRunning(false);
    }
  };

  return (
    <div className="eval-dashboard">
      <h2>üìä Dashboard d'√âvaluation IA</h2>
      
      <div className="eval-actions">
        <button 
          onClick={runEmailEval}
          disabled={isRunning}
        >
          {isRunning ? '√âvaluation...' : 'Tester Qualit√© Emails'}
        </button>
        
        <button 
          onClick={compareModels}
          disabled={isRunning}
        >
          {isRunning ? 'Comparaison...' : 'Comparer Mod√®les'}
        </button>
      </div>

      {results && (
        <div className="results-section">
          <h3>R√©sultats</h3>
          
          {results.successRate !== undefined ? (
            // R√©sultats d'√©valuation simple
            <div className="eval-results">
              <div className="metric">
                <span>Taux de R√©ussite:</span>
                <strong>{results.successRate.toFixed(1)}%</strong>
              </div>
              <div className="metric">
                <span>Tests Pass√©s:</span>
                <strong>{results.passed}/{results.totalTests}</strong>
              </div>
              <div className="metric">
                <span>Tests √âchou√©s:</span>
                <strong>{results.failed}</strong>
              </div>
            </div>
          ) : (
            // R√©sultats de comparaison
            <div className="comparison-results">
              <div className="winner">
                <h4>üèÜ Gagnant: {results.comparison.winner}</h4>
                <p>Diff√©rence: {results.comparison.difference.toFixed(1)}%</p>
              </div>
              
              <div className="model-scores">
                <div className="model-score">
                  <h5>GPT-4o</h5>
                  <span>{results['gpt-4o']?.successRate.toFixed(1)}%</span>
                </div>
                <div className="model-score">
                  <h5>GPT-4o-mini</h5>
                  <span>{results['gpt-4o-mini']?.successRate.toFixed(1)}%</span>
                </div>
              </div>
            </div>
          )}
        </div>
      )}

      <div className="evals-list">
        <h3>√âvaluations Existantes</h3>
        {evals.map(eval_ => (
          <div key={eval_.id} className="eval-item">
            <h4>{eval_.name}</h4>
            <p>Cr√©√©: {new Date(eval_.created_at * 1000).toLocaleDateString()}</p>
            <button onClick={() => viewEvalDetails(eval_.id)}>
              Voir D√©tails
            </button>
          </div>
        ))}
      </div>
    </div>
  );
}
```

## √âvaluation Continue

### Configuration d'√âvaluation Automatique

```javascript
// Configurer une √©valuation continue
const continuousEval = await emailEvalsAPI.setupContinuousEval({
  name: 'Email Quality Monitor',
  testData: [
    { prompt: 'Email standard', expected_tone: 'professional' },
    { prompt: 'Email urgent', expected_tone: 'urgent' }
  ],
  interval: 3600000, // 1 heure
  minSuccessRate: 85 // Alerter si < 85%
});

// √âcouter les r√©sultats
window.addEventListener('email-eval-completed', (event) => {
  const { results } = event.detail;
  console.log('√âvaluation automatique:', results.successRate);
  
  // Mettre √† jour le dashboard
  updateQualityMetrics(results);
});

// √âcouter les alertes qualit√©
window.addEventListener('email-quality-alert', (event) => {
  const { message, results } = event.detail;
  
  // Afficher notification
  showNotification({
    type: 'warning',
    title: 'Alerte Qualit√©',
    message: message
  });
  
  // Envoyer alerte par email
  emailAPI.send({
    to: 'admin@company.com',
    subject: 'Alerte Qualit√© IA',
    text: `La qualit√© des emails g√©n√©r√©s a chut√©: ${message}`
  });
});
```

## Types d'√âvaluateurs

### String Check
```javascript
{
  type: 'string_check',
  name: 'Exact Match',
  input: '{{sample.output_text}}',
  reference: '{{item.expected}}',
  operation: 'eq' // 'eq', 'contains', 'starts_with', 'ends_with'
}
```

### Label Model
```javascript
{
  type: 'label_model',
  model: 'gpt-4o',
  name: 'Quality Grader',
  input: [
    {
      role: 'developer',
      content: 'Rate this email as "good" or "poor"'
    },
    {
      role: 'user',
      content: 'Email: {{sample.output_text}}'
    }
  ],
  passing_labels: ['good'],
  labels: ['good', 'poor']
}
```

## Analyse des R√©sultats

```javascript
// Analyser les r√©sultats d√©taill√©s
const analyzeEvalResults = async (evalId, runId) => {
  const outputItems = await evalsAPI.runs.getOutputItems(evalId, runId);
  
  const analysis = {
    totalItems: outputItems.data.length,
    passedItems: outputItems.data.filter(item => item.status === 'pass'),
    failedItems: outputItems.data.filter(item => item.status === 'fail'),
    averageScore: 0,
    commonFailures: []
  };
  
  // Calculer le score moyen
  const scores = outputItems.data.map(item => 
    item.results.reduce((sum, result) => sum + result.score, 0) / item.results.length
  );
  analysis.averageScore = scores.reduce((sum, score) => sum + score, 0) / scores.length;
  
  // Identifier les √©checs communs
  const failureReasons = analysis.failedItems.map(item => 
    item.results.filter(result => !result.passed).map(result => result.name)
  ).flat();
  
  analysis.commonFailures = [...new Set(failureReasons)];
  
  return analysis;
};
```

## Bonnes Pratiques

### 1. Tests Repr√©sentatifs
```javascript
// Cr√©er des tests qui couvrent tous les cas d'usage
const comprehensiveTests = [
  { type: 'welcome', tone: 'friendly', length: 'short' },
  { type: 'reminder', tone: 'professional', length: 'medium' },
  { type: 'complaint', tone: 'empathetic', length: 'long' },
  { type: 'promotion', tone: 'persuasive', length: 'medium' }
];
```

### 2. M√©triques Multiples
```javascript
const multiMetricEval = {
  criteria: [
    'tone_accuracy',
    'length_appropriateness', 
    'grammar_correctness',
    'professional_language',
    'call_to_action_presence'
  ]
};
```

### 3. Monitoring Continu
```javascript
// Surveiller la d√©rive des performances
const monitorPerformanceDrift = async () => {
  const recentRuns = await evalsAPI.runs.list(evalId, { 
    limit: 10, 
    order: 'desc' 
  });
  
  const successRates = recentRuns.data.map(run => 
    (run.result_counts.passed / run.result_counts.total) * 100
  );
  
  const trend = calculateTrend(successRates);
  
  if (trend < -5) { // Baisse de 5%
    alertPerformanceDrift(trend);
  }
};
```

---

**Version** : 2.2  
**Statut** : ‚úÖ Production Ready avec Evals