"""
Test Redis LangCache - D√©monstration
Cache s√©mantique pour IA CESEDA
"""
from src.backend.services.redis_langcache import redis_langcache
from src.backend.services.ceseda_semantic_ai import ceseda_semantic
import json

def demo_langcache():
    """D√©monstration compl√®te LangCache"""
    
    print("üöÄ Test Redis LangCache - IA CESEDA")
    print("=" * 50)
    
    if not redis_langcache.enabled:
        print("‚ùå LangCache non configur√© (v√©rifiez .env)")
        return
    
    # 1. Test sauvegarde
    print("\n1Ô∏è‚É£ Test sauvegarde...")
    saved = redis_langcache.set(
        prompt="Recours pr√©fecture d√©lai respect√© documents complets",
        response=json.dumps({
            "success_probability": 0.92,
            "confidence": 0.95,
            "factors": ["d√©lai_ok", "docs_complets"]
        })
    )
    print(f"Sauvegarde: {'‚úÖ OK' if saved else '‚ùå Erreur'}")
    
    # 2. Test recherche s√©mantique
    print("\n2Ô∏è‚É£ Test recherche s√©mantique...")
    results = redis_langcache.search(
        prompt="Recours avec d√©lai OK et documents OK",
        similarity_threshold=0.8
    )
    print(f"R√©sultats trouv√©s: {len(results)}")
    
    if results:
        best = results[0]
        print(f"Meilleur match: {best.get('similarity', 0):.2f} similarit√©")
        print(f"R√©ponse: {best.get('response', 'N/A')[:100]}...")
    
    # 3. Test pr√©diction avec cache
    print("\n3Ô∏è‚É£ Test pr√©diction IA s√©mantique...")
    prediction = ceseda_semantic.predict_with_semantic_cache(
        "Recours urgent d√©lai respect√©"
    )
    
    print(f"Source: {prediction.get('source', 'unknown')}")
    print(f"Cache hit: {prediction.get('cache_hit', False)}")
    print(f"Probabilit√© succ√®s: {prediction.get('success_probability', 0):.2f}")
    
    print("\n‚úÖ D√©monstration termin√©e!")

if __name__ == "__main__":
    demo_langcache()