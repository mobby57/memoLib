# ğŸ¤– Installation Ollama - IA Locale

## Pourquoi Ollama ?

- âœ… **Gratuit** : Aucun coÃ»t d'API
- âœ… **PrivÃ©** : DonnÃ©es 100% locales, aucune fuite
- âœ… **Performant** : ModÃ¨les optimisÃ©s (Llama 3, Mistral)
- âœ… **Offline** : Fonctionne sans internet

---

## ğŸš€ Installation (5 minutes)

### 1. TÃ©lÃ©charger Ollama

**Windows** :
```powershell
# TÃ©lÃ©chargez depuis:
https://ollama.com/download/windows

# Ou utilisez winget:
winget install Ollama.Ollama
```

### 2. VÃ©rifier l'installation

```powershell
ollama --version
```

### 3. TÃ©lÃ©charger un modÃ¨le

**RecommandÃ© : Llama 3.2 (3B)** - Rapide, excellent pour l'extraction d'informations :
```powershell
ollama pull llama3.2:3b
```

**Alternative : Mistral (7B)** - Plus puissant, plus lent :
```powershell
ollama pull mistral:7b
```

**Alternative : Llama 3.1 (8B)** - Ã‰quilibrÃ© :
```powershell
ollama pull llama3.1:8b
```

### 4. Tester

```powershell
ollama run llama3.2:3b "Bonjour, rÃ©sume le droit CESEDA"
```

---

## âš™ï¸ Configuration

### DÃ©marrer le serveur Ollama (automatique)

Ollama dÃ©marre automatiquement au dÃ©marrage de Windows.

**VÃ©rifier si actif** :
```powershell
curl http://localhost:11434
```

**RÃ©sultat attendu** : `Ollama is running`

### ArrÃªter/RedÃ©marrer

```powershell
# ArrÃªter
Stop-Process -Name ollama -Force

# RedÃ©marrer (ouvre automatiquement)
ollama serve
```

---

## ğŸ¯ Utilisation dans IA Poste Manager

### 1. Variables d'environnement

Ajoutez dans `.env.local` :
```env
# Ollama Configuration
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b
```

### 2. Tester l'intÃ©gration

```powershell
npm run ai:test
```

---

## ğŸ“Š ModÃ¨les RecommandÃ©s

| ModÃ¨le | Taille | RAM | Vitesse | Usage |
|--------|--------|-----|---------|-------|
| **llama3.2:3b** | 2 GB | 8 GB | âš¡âš¡âš¡ | Extraction rapide |
| **llama3.1:8b** | 4.7 GB | 16 GB | âš¡âš¡ | Ã‰quilibrÃ© |
| **mistral:7b** | 4.1 GB | 16 GB | âš¡âš¡ | Analyse juridique |
| **llama3:70b** | 40 GB | 64 GB | âš¡ | Maximum qualitÃ© |

### Changement de modÃ¨le

```powershell
# TÃ©lÃ©charger
ollama pull mistral:7b

# Changer dans .env.local
OLLAMA_MODEL=mistral:7b
```

---

## ğŸ” FonctionnalitÃ©s IA Locale

**Extraction automatique** :
- ğŸ“‹ Type de demande CESEDA (titre de sÃ©jour, visa, naturalisation, etc.)
- ğŸ“… Dates et dÃ©lais importants
- ğŸ“ Documents nÃ©cessaires
- âš ï¸ Risques juridiques
- ğŸ¯ Actions recommandÃ©es
- ğŸ“Š Niveau d'urgence

**Analyse avancÃ©e** :
- DÃ©tection des situations OQTF
- Identification des recours possibles
- Suggestion de procÃ©dures
- Extraction des informations personnelles

---

## ğŸ†˜ DÃ©pannage

### Ollama ne dÃ©marre pas

```powershell
# VÃ©rifier les logs
Get-EventLog -LogName Application -Source Ollama -Newest 10

# RÃ©installer
winget uninstall Ollama.Ollama
winget install Ollama.Ollama
```

### Le modÃ¨le est trop lent

Utilisez un modÃ¨le plus petit :
```powershell
ollama pull llama3.2:3b
```

### Erreur de mÃ©moire

Fermez d'autres applications ou utilisez un modÃ¨le plus petit.

---

## ğŸ“š Ressources

- [Ollama Documentation](https://ollama.com/docs)
- [Liste des modÃ¨les](https://ollama.com/library)
- [Llama 3 Guide](https://ollama.com/library/llama3)
