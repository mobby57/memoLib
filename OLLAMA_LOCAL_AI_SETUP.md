# ü§ñ Configuration IA Locale avec Ollama

## üéØ Pourquoi l'IA Locale?

### ‚úÖ Avantages pour un Cabinet d'Avocat

1. **Confidentialit√© Absolue** üîí
   - Donn√©es juridiques sensibles restent sur votre serveur
   - Aucune transmission √† des services tiers (OpenAI, Anthropic, etc.)
   - Conformit√© RGPD garantie
   - Secret professionnel respect√©

2. **Co√ªt Z√©ro** üí∞
   - Pas de frais d'API (Claude co√ªte ~$15-60/million tokens)
   - Traitement illimit√© d'emails
   - √âconomie de plusieurs centaines d'euros par mois

3. **Contr√¥le Total** ‚öôÔ∏è
   - Choix du mod√®le selon vos besoins
   - Personnalisation possible avec fine-tuning
   - Pas de limite de requ√™tes
   - Fonctionne hors-ligne

4. **Performance** ‚ö°
   - R√©ponses rapides (pas de latence r√©seau)
   - Traitement en local
   - Scalable selon votre mat√©riel

---

## üì• Installation Ollama

### Windows

1. **T√©l√©charger Ollama**
   - Aller sur [ollama.com](https://ollama.com/download/windows)
   - T√©l√©charger `OllamaSetup.exe`
   - Installer (installation simple, comme n'importe quel logiciel)

2. **V√©rifier l'installation**
   ```powershell
   ollama --version
   ```

3. **D√©marrer Ollama** (se lance automatiquement au d√©marrage)
   - L'ic√¥ne Ollama appara√Æt dans la barre des t√¢ches
   - Le serveur tourne sur `http://localhost:11434`

### Linux / macOS

```bash
curl -fsSL https://ollama.com/install.sh | sh
```

---

## üß† Choix du Mod√®le IA

### Pour le Juridique (CESEDA, R√©ponses Emails)

Voici les meilleurs mod√®les test√©s pour le contexte juridique fran√ßais:

#### ü•á **Recommand√© : Llama 3.2 (3B)** - Rapide et Efficace
```bash
ollama pull llama3.2:latest
```
- ‚úÖ Excellent pour les r√©ponses emails
- ‚úÖ Rapide (m√™me sur CPU moyen)
- ‚úÖ Bon en fran√ßais
- ‚úÖ 3GB RAM minimum
- üìä Performance: **8/10**

#### ü•à **Alternative : Mistral (7B)** - Meilleur en Fran√ßais
```bash
ollama pull mistral:latest
```
- ‚úÖ Excellent fran√ßais (d√©velopp√© en France)
- ‚úÖ Tr√®s bon en contexte juridique
- ‚úÖ Raisonnement solide
- ‚ö†Ô∏è Plus lourd (8GB RAM recommand√©)
- üìä Performance: **9/10**

#### ü•â **Haute Performance : Llama 3.1 (8B)** - Le Plus Puissant
```bash
ollama pull llama3.1:latest
```
- ‚úÖ Meilleure compr√©hension du contexte
- ‚úÖ R√©ponses plus nuanc√©es
- ‚úÖ Excellent pour extractions complexes
- ‚ö†Ô∏è N√©cessite 16GB RAM
- üìä Performance: **10/10**

#### üöÄ **Expert : Qwen 2.5 (14B)** - Pour Serveur Puissant
```bash
ollama pull qwen2.5:14b
```
- ‚úÖ Tr√®s performant en fran√ßais
- ‚úÖ Excellent en extraction de donn√©es structur√©es
- ‚úÖ G√®re bien le contexte juridique
- ‚ö†Ô∏è N√©cessite 32GB RAM
- üìä Performance: **10/10**

---

## ‚öôÔ∏è Configuration dans iaPostemanage

### 1. V√©rifier Ollama

```powershell
# V√©rifier que le serveur tourne
curl http://localhost:11434/api/tags
```

R√©ponse attendue:
```json
{
  "models": [
    {
      "name": "llama3.2:latest",
      "modified_at": "...",
      "size": 2019393189
    }
  ]
}
```

### 2. Tester le Mod√®le

```powershell
ollama run llama3.2:latest "√âcris une r√©ponse professionnelle √† un client qui demande un rendez-vous pour un dossier CESEDA urgent."
```

### 3. Configuration .env

Votre fichier `.env` est d√©j√† configur√©:

```env
OLLAMA_ENABLED="true"
OLLAMA_URL="http://localhost:11434"
OLLAMA_MODEL="llama3.2:latest"
```

### 4. Configuration par Tenant

Chaque cabinet peut avoir sa propre configuration dans l'interface admin:

- **URL Ollama**: `http://localhost:11434` (ou IP serveur distant)
- **Mod√®le**: `llama3.2:latest`, `mistral:latest`, etc.
- **Activ√©**: `true`

---

## üß™ Test de G√©n√©ration de R√©ponse

### Test Simple

```typescript
// Test dans Node.js
const response = await fetch('http://localhost:11434/api/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({
    model: 'llama3.2:latest',
    prompt: 'G√©n√®re une r√©ponse professionnelle pour un avocat sp√©cialis√© en CESEDA qui re√ßoit un email d\'un client demandant un rendez-vous urgent pour un titre de s√©jour.',
    stream: false
  })
});

const data = await response.json();
console.log(data.response);
```

### Test avec l'API Email

1. **Lancer le monitoring int√©gr√©**:
   ```powershell
   npm run email:monitor:integrated
   ```

2. **V√©rifier les logs**:
   - Chercher `ü§ñ G√©n√©ration avec IA locale Ollama...`
   - Chercher `‚úÖ Brouillon g√©n√©r√© localement`

3. **Tester la g√©n√©ration de r√©ponse**:
   ```bash
   # Via API
   curl -X POST http://localhost:3000/api/lawyer/emails/ai-response \
     -H "Content-Type: application/json" \
     -d '{
       "emailId": "your-email-id",
       "action": "generate"
     }'
   ```

---

## üéõÔ∏è Optimisation des Performances

### Configuration Ollama Avanc√©e

Cr√©er un fichier `modelfile` pour personnaliser:

```modelfile
FROM llama3.2:latest

# Param√®tres pour le juridique fran√ßais
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_predict 1024

# Prompt syst√®me pour CESEDA
SYSTEM """
Tu es un assistant juridique sp√©cialis√© en droit des √©trangers (CESEDA).
Tu r√©diges des r√©ponses professionnelles pour un cabinet d'avocat fran√ßais.
Ton ton est formel, respectueux et conforme aux usages juridiques fran√ßais.
Tu respectes le secret professionnel et la confidentialit√© des clients.
"""
```

Cr√©er le mod√®le personnalis√©:
```bash
ollama create ceseda-assistant -f ./modelfile
```

Utiliser le mod√®le personnalis√©:
```env
OLLAMA_MODEL="ceseda-assistant"
```

### Acc√©l√©ration GPU (Optionnel)

Si vous avez une carte NVIDIA:

1. Installer CUDA Toolkit 12.x
2. Ollama d√©tecte automatiquement le GPU
3. V√©rifier:
   ```powershell
   ollama list
   # GPU device sera affich√©
   ```

Performance attendue:
- **CPU**: 10-30 tokens/sec
- **GPU (RTX 3060)**: 50-100 tokens/sec
- **GPU (RTX 4090)**: 150-300 tokens/sec

---

## üìä Comparaison IA Cloud vs Locale

| Crit√®re | Claude API (Cloud) | Ollama (Local) |
|---------|-------------------|----------------|
| **Confidentialit√©** | ‚ö†Ô∏è Donn√©es envoy√©es √† Anthropic | ‚úÖ 100% local |
| **Co√ªt** | üí∞ $15-60/million tokens | ‚úÖ Gratuit |
| **Performance** | ‚ö° Tr√®s rapide | ‚ö° Rapide (selon mat√©riel) |
| **Qualit√©** | üåü Excellente | üåü Bonne √† excellente |
| **Hors-ligne** | ‚ùå Non | ‚úÖ Oui |
| **RGPD** | ‚ö†Ô∏è Transfert hors UE | ‚úÖ Conforme |
| **Limite** | ‚ö†Ô∏è Rate limits | ‚úÖ Illimit√© |

---

## üîß D√©pannage

### Erreur "Ollama API error: Failed to fetch"

**Solution:**
```powershell
# V√©rifier que Ollama tourne
Get-Process ollama

# Red√©marrer Ollama
# 1. Fermer l'ic√¥ne dans la barre des t√¢ches
# 2. Chercher "Ollama" dans le menu D√©marrer
# 3. Relancer
```

### R√©ponses en Anglais

**Solution**: Forcer le fran√ßais dans le prompt
```typescript
const userPrompt = `IMPORTANT: R√©ponds UNIQUEMENT en fran√ßais.

G√©n√®re une r√©ponse professionnelle...`;
```

### Mod√®le Trop Lent

**Solutions:**
1. Utiliser un mod√®le plus petit: `llama3.2:3b`
2. R√©duire `num_predict` (moins de tokens g√©n√©r√©s)
3. Upgrader la RAM/CPU
4. Utiliser un GPU

### Erreur "Model not found"

```powershell
# Lister les mod√®les install√©s
ollama list

# T√©l√©charger le mod√®le manquant
ollama pull llama3.2:latest
```

---

## üöÄ D√©ploiement Production

### Serveur D√©di√© Ollama

Pour un cabinet avec plusieurs utilisateurs:

1. **Serveur Linux (Ubuntu)**:
   ```bash
   # Installation
   curl -fsSL https://ollama.com/install.sh | sh
   
   # Activer l'acc√®s r√©seau
   sudo systemctl edit ollama
   
   # Ajouter:
   [Service]
   Environment="OLLAMA_HOST=0.0.0.0:11434"
   
   # Red√©marrer
   sudo systemctl restart ollama
   ```

2. **Configuration iaPostemanage**:
   ```env
   OLLAMA_URL="http://192.168.1.100:11434"
   ```

3. **Firewall**:
   ```bash
   sudo ufw allow 11434/tcp
   ```

### Haute Disponibilit√©

Pour 100% uptime:

1. Installer Ollama sur plusieurs serveurs
2. Utiliser un load balancer (nginx):
   ```nginx
   upstream ollama_backend {
       server 192.168.1.100:11434;
       server 192.168.1.101:11434;
   }
   ```

---

## üìö Ressources

- [Ollama Documentation](https://ollama.com/docs)
- [Mod√®les disponibles](https://ollama.com/library)
- [Fine-tuning Guide](https://ollama.com/blog/fine-tuning)
- [Llama 3.2 Paper](https://ai.meta.com/llama/)
- [Mistral Documentation](https://mistral.ai/technology/)

---

## üéì Cas d'Usage Juridique

### 1. R√©ponse Email Nouveau Client

**Prompt Ollama:**
```
G√©n√®re une r√©ponse professionnelle pour un avocat sp√©cialis√© en CESEDA.

Email client:
"Bonjour, je suis en situation irr√©guli√®re et j'ai re√ßu une OQTF. 
Pouvez-vous m'aider? Je suis disponible cette semaine."

Contexte: Premier contact, urgence OQTF

R√©ponse (ton formel, propose RDV urgence, mentionne documents n√©cessaires):
```

### 2. Extraction Donn√©es Structur√©es

**Prompt:**
```
Extrais les informations de cet email en JSON:

"Mon num√©ro de dossier pr√©fecture: 2024-075-12345
Je suis joignable au 06 12 34 56 78
Mon titre de s√©jour expire le 15 mars 2026"

JSON (dates, phones, documentTypes, urgencyMarkers):
```

### 3. R√©sum√© Email

**Prompt:**
```
R√©sume en 100 caract√®res max:

"Bonjour Ma√Ætre, suite √† notre rendez-vous du 5 janvier, 
je vous envoie les documents demand√©s: copie passeport, 
justificatif domicile, et bulletin salaire d√©cembre. 
Mon dossier pr√©fecture sera trait√© le 20 janvier selon 
la convocation re√ßue hier. Merci de confirmer r√©ception."

R√©sum√©:
```

---

## üîê S√©curit√© & RGPD

### Conformit√© RGPD

‚úÖ **Ollama Local = 100% Conforme**

1. **Traitement local**: Donn√©es jamais transf√©r√©es
2. **Pas de sous-traitant**: Pas de DPA n√©cessaire
3. **Ma√Ætrise totale**: Suppression garantie
4. **Audit trail**: Logs locaux
5. **Chiffrement**: Possibilit√© de chiffrer les donn√©es au repos

### Registre des Traitements

Exemple d'entr√©e pour votre registre RGPD:

```
Traitement: G√©n√©ration automatique de r√©ponses emails
Finalit√©: Assistance r√©dactionnelle pour avocats
Base l√©gale: Int√©r√™t l√©gitime (efficacit√© cabinet)
Donn√©es: Emails clients (nom, situation juridique)
Destinataires: Aucun (traitement 100% local)
Dur√©e conservation: Selon dur√©e du dossier
Mesures s√©curit√©: IA locale (Ollama), pas de transfert externe
```

---

üéØ **Votre syst√®me est maintenant 100% confidentiel et RGPD-compliant!**
